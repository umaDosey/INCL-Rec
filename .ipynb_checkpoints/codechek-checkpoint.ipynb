{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c1a316-b89f-4819-97e6-aebcdddba301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "import networkx as nx\n",
    "from sortedcontainers import SortedList\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from utils import get_sparse_tensor, graph_rank_nodes, generate_daj_mat, generate_aug_daj_mat\n",
    "from torch.nn.init import kaiming_uniform_, xavier_normal, normal_, zeros_, ones_\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import dgl\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from info_nce import InfoNCE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import AverageMeter, get_sparse_tensor\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from dataset import AuxiliaryDataset\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fc200e-1bad-4de1-a1ed-2337dcec623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataset\n",
    "from model import get_model\n",
    "from trainer import get_trainer\n",
    "import torch\n",
    "from utils import init_run\n",
    "from tensorboardX import SummaryWriter\n",
    "from config import get_gowalla_config, get_yelp_config, get_amazon_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a4b3fc-05cb-4699-a484-3774a3044221",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "config = get_gowalla_config(device)\n",
    "dataset_config, model_config, trainer_config = config[10]\n",
    "dataset_config['path'] = dataset_config['path'][:-4] + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9f301-8384-4b70-9448-e313eece2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'ProcessedDataset', 'path': 'data/Gowalla/1', 'device': device(type='cuda', index=1), 'neg_ratio': 4}\n",
      "init dataset ProcessedDataset\n",
      "{'name': 'DOSE', 'embedding_size': 64, 'n_layers': 3, 'device': device(type='cuda', index=1), 'dropout': 0.3, 'feature_ratio': 0.6, 'aug_num': 500000, 'dataset': <dataset.ProcessedDataset object at 0x1522b74557c0>}\n"
     ]
    }
   ],
   "source": [
    "#writer = SummaryWriter(log_path)\n",
    "dataset = get_dataset(dataset_config) # datasetのクラス\n",
    "model = get_model(model_config, dataset) # modelのクラウス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ba827-f461-4ebb-84e6-5d0d865ffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c39a6-186f-4f55-873e-b4e090703663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class DOSE(BasicModel):\n",
    "    def __init__(self, model_config):\n",
    "        super(DOSE, self).__init__(model_config)\n",
    "        self.embedding_size = model_config['embedding_size']\n",
    "        self.n_layers = model_config['n_layers']\n",
    "        self.dropout = model_config['dropout']\n",
    "        self.feature_ratio = model_config['feature_ratio']  # Template%\n",
    "        self.norm_adj = self.generate_graph(model_config['dataset'])\n",
    "       \n",
    "        self.alpha = 1.\n",
    "        self.delta = model_config.get('delta', 0.99)\n",
    "        self.taugh = model_config.get('taugh', 0.2)\n",
    "        self.aug_num = model_config['aug_num']\n",
    "        # self.temper = model_config['temper']\n",
    "        self.feat_mat, self.user_map, self.item_map, self.row_sum = \\\n",
    "            self.generate_feat(model_config['dataset'],\n",
    "                               ranking_metric=model_config.get('ranking_metric', 'sort'))\n",
    "        self.update_feat_mat()\n",
    "        # self.norm_aug_adj = enerate_aug_graph(model_config['dataset'] )\n",
    "        self.embedding = nn.Embedding(self.feat_mat.shape[1], self.embedding_size)\n",
    "        \n",
    "        self.w = nn.Parameter(torch.ones([self.embedding_size], dtype=torch.float32, device=self.device))\n",
    "        normal_(self.embedding.weight, std=0.1)\n",
    "        self.to(device=self.device)\n",
    "        self.norm_aug_adj = self.generate_aug_graph(model_config['dataset'])\n",
    "        self.times = model_config.get('times', 0.1)\n",
    "        # self.aug_num = (self.times) * len(self.feat_mat)\n",
    "\n",
    "    def update_feat_mat(self):\n",
    "        row, _ = self.feat_mat.indices()\n",
    "        edge_values = torch.pow(self.row_sum[row], (self.alpha - 1.) / 2. - 0.5)\n",
    "        self.feat_mat = torch.sparse.FloatTensor(self.feat_mat.indices(), edge_values, self.feat_mat.shape).coalesce()\n",
    "\n",
    "    def feat_mat_anneal(self):\n",
    "        self.alpha *= self.delta\n",
    "        self.update_feat_mat()\n",
    "\n",
    "    def generate_graph(self, dataset):\n",
    "        return LightGCN.generate_graph(self, dataset)\n",
    "    \n",
    "\n",
    "\n",
    "    def generate_aug_graph(self, dataset):\n",
    "        # new graph after adding some interactions\n",
    "        aug_idx = self.cal_cos_sim()\n",
    "        aug_adj_mat = generate_aug_daj_mat(dataset, aug_idx)\n",
    "        degree = np.array(np.sum(aug_adj_mat, axis=1)).squeeze()\n",
    "        degree = np.maximum(1., degree)\n",
    "        d_inv = np.power(degree, -0.5) # 累乗\n",
    "        d_mat = sp.diags(d_inv, format='csr', dtype=np.float32)\n",
    "\n",
    "        norm_aug_adj = d_mat.dot(aug_adj_mat).dot(d_mat)\n",
    "        norm_aug_adj = get_sparse_tensor(norm_aug_adj, self.device)\n",
    "        return norm_aug_adj\n",
    "\n",
    "    def generate_feat(self, dataset, is_updating=False, ranking_metric=None):\n",
    "        # return adj matrix with template\n",
    "        if not is_updating:\n",
    "            if self.feature_ratio < 1.:  # ランク付後にスライス\n",
    "\n",
    "                ranked_users, ranked_items = graph_rank_nodes(dataset, ranking_metric)\n",
    "                core_users = ranked_users[:int(self.n_users * self.feature_ratio)]\n",
    "                core_items = ranked_items[:int(self.n_items * self.feature_ratio)]\n",
    "            else:\n",
    "                core_users = np.arange(self.n_users, dtype=np.int64)\n",
    "                core_items = np.arange(self.n_items, dtype=np.int64)\n",
    "\n",
    "            user_map = dict()\n",
    "            for idx, user in enumerate(core_users):\n",
    "                user_map[user] = idx\n",
    "            item_map = dict()\n",
    "            for idx, item in enumerate(core_items):\n",
    "                item_map[item] = idx\n",
    "        else:\n",
    "            user_map = self.user_map\n",
    "            item_map = self.item_map\n",
    "\n",
    "        user_dim, item_dim = len(user_map), len(item_map)\n",
    "        indices = []\n",
    "        for user, item in dataset.train_array:\n",
    "            if item in item_map:\n",
    "                indices.append([user, user_dim + item_map[item]])\n",
    "            if user in user_map:\n",
    "                indices.append([self.n_users + item, user_map[user]])\n",
    "        for user in range(self.n_users):\n",
    "            indices.append([user, user_dim + item_dim])\n",
    "        for item in range(self.n_items):\n",
    "            indices.append([self.n_users + item, user_dim + item_dim + 1])\n",
    "        feat = sp.coo_matrix((np.ones((len(indices),)), np.array(indices).T),\n",
    "                             shape=(self.n_users + self.n_items, user_dim + item_dim + 2), dtype=np.float32).tocsr()\n",
    "        row_sum = torch.tensor(np.array(np.sum(feat, axis=1)).squeeze(), dtype=torch.float32, device=self.device)\n",
    "        feat = get_sparse_tensor(feat, self.device)\n",
    "        return feat, user_map, item_map, row_sum\n",
    "\n",
    "    def inductive_rep_layer(self, feat_mat):\n",
    "        # generate embedding by using template\n",
    "        padding_tensor = torch.empty([max(self.feat_mat.shape) - self.feat_mat.shape[1], self.embedding_size],\n",
    "                                     dtype=torch.float32, device=self.device)\n",
    "        padding_features = torch.cat([self.embedding.weight, padding_tensor], dim=0)\n",
    "\n",
    "        row, column = feat_mat.indices()\n",
    "        g = dgl.graph((column, row), num_nodes=max(self.feat_mat.shape), device=self.device)\n",
    "        x = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=padding_features, rhs_data=feat_mat.values())\n",
    "        x = x[:self.feat_mat.shape[0], :]\n",
    "        return x\n",
    "\n",
    "    def get_def_rep(self):\n",
    "        # generate final embedding\n",
    "        feat_mat = NGCF.dropout_sp_mat(self, self.feat_mat)\n",
    "        representations = self.inductive_rep_layer(feat_mat)\n",
    "\n",
    "        all_layer_rep = [representations]\n",
    "        row, column = self.norm_adj.indices()\n",
    "        g = dgl.graph((column, row), num_nodes=self.norm_adj.shape[0], device=self.device)\n",
    "        for _ in range(self.n_layers):\n",
    "            representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=self.norm_adj.values())\n",
    "            all_layer_rep.append(representations)\n",
    "        all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "        final_rep = all_layer_rep.mean(dim=0)\n",
    "        return final_rep\n",
    "\n",
    "    def get_aug_rep(self, norm_aug_adj):\n",
    "        # generate final embedding on aug-graph\n",
    "        feat_mat = NGCF.dropout_sp_mat(self, self.feat_mat)\n",
    "        representations = self.inductive_rep_layer(feat_mat)\n",
    "        \n",
    "        all_layer_rep = [representations]\n",
    "        row, column = norm_aug_adj.indices()\n",
    "        g = dgl.graph((column, row), num_nodes=norm_aug_adj.shape[0], device=self.device)\n",
    "        for _ in range(self.n_layers):\n",
    "            representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=norm_aug_adj.values())\n",
    "            all_layer_rep.append(representations)\n",
    "        all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "        final_rep = all_layer_rep.mean(dim=0)\n",
    "        return final_rep\n",
    "\n",
    "    def cal_cos_sim(self):\n",
    "        # calculate cosine similarity with user embeddings and item embeddings (on CPU)\n",
    "        rep = self.get_def_rep()\n",
    "        all_users_r = rep[:self.n_users, :]\n",
    "        all_items_r = rep[self.n_users:, :]\n",
    "        \n",
    "        all_users_r = all_users_r.to('cpu').detach().numpy().copy()\n",
    "        all_items_r = all_items_r.to('cpu').detach().numpy().copy()\n",
    "        \n",
    "        x = cosine_similarity(all_users_r, all_items_r)\n",
    "        cos_mat = torch.from_numpy(x.astype(np.float32)).clone()\n",
    "        cos_mat.to(device=self.device)\n",
    "        cos_sim = torch.reshape(cos_mat, (1, -1))\n",
    "        _, idx = torch.topk(cos_sim, self.aug_num)\n",
    "        #idx = idx.tolist()S\n",
    "        aug_idx = [[int(torch.div(idx[0][i], self.n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], self.n_items)))] for i in range(self.aug_num)]\n",
    "        \n",
    "        return aug_idx  # return list [user_id, item_id]\n",
    "    \n",
    "    def cal_cos_sim_v2(self):\n",
    "        # calculate cosine similarity with user embeddings and item embeddings (on GPU)\n",
    "        rep = self.get_def_rep()\n",
    "        all_users_r = rep[:self.n_users, :]\n",
    "        all_items_r = rep[self.n_users:, :]\n",
    "        \n",
    "        cos_sim = pairwise_cosine_similarity(all_users_r, all_items_r)\n",
    "        cos_sim = torch.reshape(cos_sim, (1, -1))\n",
    "        _, idx = torch.topk(cos_sim, self.aug_num)\n",
    "        aug_idx = [[int(torch.div(idx[0][i], self.n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], self.n_items)))] for i in range(self.aug_num)]\n",
    "\n",
    "        return aug_idx  # return list [user_id, item_id]\n",
    "\n",
    "\n",
    "    def cal_loss(self, users_r, aug_users_r):\n",
    "        # calcrate ssl-loss(InfoNCE)\n",
    "        loss = InfoNCE(negative_mode='paired')\n",
    "        \n",
    "        query = users_r / self.taugh\n",
    "        positive_key = aug_users_r\n",
    "        negative_keys = aug_users_r.unsqueeze(1)\n",
    "        \n",
    "        contrasive_loss = loss(query, positive_key, negative_keys)\n",
    "        return contrasive_loss\n",
    "\n",
    "    def bpr_forward(self, users, pos_items, neg_items):\n",
    "        # 普通の埋め込み\n",
    "        rep = self.get_def_rep()\n",
    "        users_r = rep[users, :]\n",
    "        pos_items_r, neg_items_r = rep[self.n_users + pos_items, :], rep[self.n_users + neg_items, :]\n",
    "        # AUG後の埋め込み\n",
    "        \n",
    "        aug_rep = self.get_aug_rep(self.norm_aug_adj)\n",
    "        aug_users_r = aug_rep[users, :]\n",
    "        l2_norm_sq = torch.norm(users_r, p=2, dim=1) ** 2 + torch.norm(pos_items_r, p=2, dim=1) ** 2 \\\n",
    "                     + torch.norm(neg_items_r, p=2, dim=1) ** 2\n",
    "        # Contrasive loss\n",
    "        contrasive_loss = self.cal_loss(users_r, aug_users_r)\n",
    "        return users_r, pos_items_r, neg_items_r, l2_norm_sq, contrasive_loss\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, users):\n",
    "        rep = self.get_def_rep()\n",
    "        users_r = rep[users, :]\n",
    "        all_items_r = rep[self.n_users:, :]\n",
    "        scores = torch.mm(users_r, all_items_r.t())\n",
    "        return scores\n",
    "    \n",
    "    def save(self, path):\n",
    "        params = {'sate_dict': self.state_dict(), 'user_map': self.user_map,\n",
    "                  'item_map': self.item_map, 'alpha': self.alpha}\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        params = torch.load(path, map_location=self.device)\n",
    "        self.load_state_dict(params['sate_dict'])\n",
    "        self.user_map = params['user_map']\n",
    "        self.item_map = params['item_map']\n",
    "        self.alpha = params['alpha']\n",
    "        self.feat_mat, _, _, self.row_sum = self.generate_feat(self.config['dataset'], is_updating=True)\n",
    "        self.update_feat_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685eef4c-bf6b-46ba-8b6f-466beb6ec815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_feat_mat():\n",
    "    row, _ = feat_mat.indices()\n",
    "    edge_values = torch.pow(row_sum[row], (alpha - 1.) / 2. - 0.5)\n",
    "    feat_mat = torch.sparse.FloatTensor(feat_mat.indices(), edge_values, feat_mat.shape).coalesce()\n",
    "\n",
    "def feat_mat_anneal():\n",
    "    alpha *= delta\n",
    "    self.update_feat_mat()\n",
    "\n",
    "\n",
    "def generate_graph( dataset):\n",
    "    adj_mat = generate_daj_mat(dataset)\n",
    "    degree = np.array(np.sum(adj_mat, axis=1)).squeeze()\n",
    "    degree = np.maximum(1., degree)\n",
    "    d_inv = np.power(degree, -0.5) # 累乗\n",
    "    d_mat = sp.diags(d_inv, format='csr', dtype=np.float32)\n",
    "\n",
    "    norm_adj = d_mat.dot(adj_mat).dot(d_mat)\n",
    "    norm_adj = get_sparse_tensor(norm_adj, device)\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "\n",
    "def generate_aug_graph(dataset):\n",
    "    # new graph after adding some interactions\n",
    "    aug_idx = cal_cos_sim()\n",
    "    aug_adj_mat = generate_aug_daj_mat(dataset, aug_idx)\n",
    "    degree = np.array(np.sum(aug_adj_mat, axis=1)).squeeze()\n",
    "    degree = np.maximum(1., degree)\n",
    "    d_inv = np.power(degree, -0.5) # 累乗\n",
    "    d_mat = sp.diags(d_inv, format='csr', dtype=np.float32)\n",
    "\n",
    "    norm_aug_adj = d_mat.dot(aug_adj_mat).dot(d_mat)\n",
    "    norm_aug_adj = get_sparse_tensor(norm_aug_adj, device)\n",
    "    return norm_aug_adj\n",
    "\n",
    "def generate_feat(dataset, is_updating=False, ranking_metric=None):\n",
    "    # return adj matrix with template\n",
    "    if not is_updating:\n",
    "        if feature_ratio < 1.:  # ランク付後にスライス\n",
    "\n",
    "            ranked_users, ranked_items = graph_rank_nodes(dataset, ranking_metric)\n",
    "            core_users = ranked_users[:int(n_users * feature_ratio)]\n",
    "            core_items = ranked_items[:int(n_items * feature_ratio)]\n",
    "        else:\n",
    "            core_users = np.arange(n_users, dtype=np.int64)\n",
    "            core_items = np.arange(n_items, dtype=np.int64)\n",
    "\n",
    "        user_map = dict()\n",
    "        for idx, user in enumerate(core_users):\n",
    "            user_map[user] = idx\n",
    "        item_map = dict()\n",
    "        for idx, item in enumerate(core_items):\n",
    "            item_map[item] = idx\n",
    "    else:\n",
    "        user_map = user_map\n",
    "        item_map = item_map\n",
    "\n",
    "    user_dim, item_dim = len(user_map), len(item_map)\n",
    "    indices = []\n",
    "    for user, item in dataset.train_array:\n",
    "        if item in item_map:\n",
    "            indices.append([user, user_dim + item_map[item]])\n",
    "        if user in user_map:\n",
    "            indices.append([n_users + item, user_map[user]])\n",
    "    for user in range(n_users):\n",
    "        indices.append([user, user_dim + item_dim])\n",
    "    for item in range(n_items):\n",
    "        indices.append([n_users + item, user_dim + item_dim + 1])\n",
    "    feat = sp.coo_matrix((np.ones((len(indices),)), np.array(indices).T),\n",
    "                         shape=(n_users + n_items, user_dim + item_dim + 2), dtype=np.float32).tocsr()\n",
    "    row_sum = torch.tensor(np.array(np.sum(feat, axis=1)).squeeze(), dtype=torch.float32, device=device)\n",
    "    feat = get_sparse_tensor(feat, device)\n",
    "    return feat, user_map, item_map, row_sum\n",
    "\n",
    "def inductive_rep_layer(feat_mat):\n",
    "    # generate embedding by using template\n",
    "    padding_tensor = torch.empty([max(feat_mat.shape) - feat_mat.shape[1], embedding_size],\n",
    "                                 dtype=torch.float32, device=device)\n",
    "    padding_features = torch.cat([embedding.weight, padding_tensor], dim=0)\n",
    "\n",
    "    row, column = feat_mat.indices()\n",
    "    g = dgl.graph((column, row), num_nodes=max(feat_mat.shape), device=device)\n",
    "    x = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=padding_features, rhs_data=feat_mat.values())\n",
    "    x = x[:feat_mat.shape[0], :]\n",
    "    return x\n",
    "\n",
    "def get_def_rep():\n",
    "    # generate final embedding\n",
    "    #feat_mat = NGCF.dropout_sp_mat(feat_mat)\n",
    "    representations = inductive_rep_layer(feat_mat)\n",
    "\n",
    "    all_layer_rep = [representations]\n",
    "    row, column = norm_adj.indices()\n",
    "    g = dgl.graph((column, row), num_nodes=norm_adj.shape[0], device=device)\n",
    "    for _ in range(n_layers):\n",
    "        representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=norm_adj.values())\n",
    "        all_layer_rep.append(representations)\n",
    "    all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "    final_rep = all_layer_rep.mean(dim=0)\n",
    "    return final_rep\n",
    "\n",
    "def get_aug_rep(norm_aug_adj):\n",
    "    # generate final embedding on aug-graph\n",
    "    #feat_mat = NGCF.dropout_sp_mat(self, self.feat_mat)\n",
    "    representations = inductive_rep_layer(feat_mat)\n",
    "\n",
    "    all_layer_rep = [representations]\n",
    "    row, column = norm_aug_adj.indices()\n",
    "    g = dgl.graph((column, row), num_nodes=norm_aug_adj.shape[0], device=device)\n",
    "    for _ in range(self.n_layers):\n",
    "        representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=norm_aug_adj.values())\n",
    "        all_layer_rep.append(representations)\n",
    "    all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "    final_rep = all_layer_rep.mean(dim=0)\n",
    "    return final_rep\n",
    "\n",
    "def cal_cos_sim():\n",
    "    # calculate cosine similarity with user embeddings and item embeddings (on CPU)\n",
    "    rep = get_def_rep()\n",
    "    all_users_r = rep[:n_users, :]\n",
    "    all_items_r = rep[n_users:, :]\n",
    "\n",
    "    all_users_r = all_users_r.to('cpu').detach().numpy().copy()\n",
    "    all_items_r = all_items_r.to('cpu').detach().numpy().copy()\n",
    "\n",
    "    x = cosine_similarity(all_users_r, all_items_r)\n",
    "    cos_mat = torch.from_numpy(x.astype(np.float32)).clone()\n",
    "    cos_mat.to(device=device)\n",
    "    cos_sim = torch.reshape(cos_mat, (1, -1))\n",
    "    _, idx = torch.topk(cos_sim, aug_num)\n",
    "    #idx = idx.tolist()S\n",
    "    aug_idx = [[int(torch.div(idx[0][i], n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], n_items)))] for i in range(aug_num)]\n",
    "\n",
    "    return aug_idx  # return list [user_id, item_id]\n",
    "\n",
    "def cal_cos_sim_v2():\n",
    "    # calculate cosine similarity with user embeddings and item embeddings (on GPU)\n",
    "    rep = get_def_rep()\n",
    "    all_users_r = rep[:n_users, :]\n",
    "    all_items_r = rep[n_users:, :]\n",
    "\n",
    "    cos_sim = pairwise_cosine_similarity(all_users_r, all_items_r)\n",
    "    cos_sim = torch.reshape(cos_sim, (1, -1))\n",
    "    _, idx = torch.topk(cos_sim, aug_num)\n",
    "    aug_idx = [[int(torch.div(idx[0][i], n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], n_items)))] for i in range(aug_num)]\n",
    "\n",
    "    return aug_idx  # return list [user_id, item_id]\n",
    "\n",
    "\n",
    "def cal_loss(users_r, aug_users_r):\n",
    "    # calcrate ssl-loss(InfoNCE)\n",
    "    loss = InfoNCE(negative_mode='paired')\n",
    "\n",
    "    query = users_r / taugh\n",
    "    positive_key = aug_users_r\n",
    "    negative_keys = aug_users_r.unsqueeze(1)\n",
    "\n",
    "    contrasive_loss = loss(query, positive_key, negative_keys)\n",
    "    return contrasive_loss\n",
    "\n",
    "def bpr_forward(self, users, pos_items, neg_items):\n",
    "    # 普通の埋め込み\n",
    "    rep = get_def_rep()\n",
    "    users_r = rep[users, :]\n",
    "    pos_items_r, neg_items_r = rep[n_users + pos_items, :], rep[n_users + neg_items, :]\n",
    "    # AUG後の埋め込み\n",
    "\n",
    "    aug_rep = get_aug_rep(norm_aug_adj)\n",
    "    aug_users_r = aug_rep[users, :]\n",
    "    l2_norm_sq = torch.norm(users_r, p=2, dim=1) ** 2 + torch.norm(pos_items_r, p=2, dim=1) ** 2 \\\n",
    "                 + torch.norm(neg_items_r, p=2, dim=1) ** 2\n",
    "    # Contrasive loss\n",
    "    contrasive_loss = cal_loss(users_r, aug_users_r)\n",
    "    return users_r, pos_items_r, neg_items_r, l2_norm_sq, contrasive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad38a8-73ef-49c3-b476-6c121a51d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_tensor(mat, device):\n",
    "    coo = mat.tocoo()\n",
    "    indexes = np.stack([coo.row, coo.col], axis=0)\n",
    "    indexes = torch.tensor(indexes, dtype=torch.int64, device=device)\n",
    "    data = torch.tensor(coo.data, dtype=torch.float32, device=device)\n",
    "    sp_tensor = torch.sparse.FloatTensor(indexes, data, torch.Size(coo.shape)).coalesce()\n",
    "    return sp_tensor\n",
    "\n",
    "\n",
    "def generate_daj_mat(dataset): # Sparseなadj\n",
    "    train_array = np.array(dataset.train_array)\n",
    "    users, items = train_array[:, 0], train_array[:, 1]\n",
    "    row = np.concatenate([users, items + dataset.n_users], axis=0)\n",
    "    column = np.concatenate([items + dataset.n_users, users], axis=0)\n",
    "    adj_mat = sp.coo_matrix((np.ones(row.shape), np.stack([row, column], axis=0)),\n",
    "                            shape=(dataset.n_users + dataset.n_items, dataset.n_users + dataset.n_items),\n",
    "                            dtype=np.float32).tocsr()\n",
    "    return adj_mat\n",
    "def generate_aug_daj_mat(dataset, aug_idx): # Sparseなadj\n",
    "    # generate adj matrix on aug-graph\n",
    "    train_array = dataset.train_array\n",
    "    train_array.extend(aug_idx)\n",
    "    train_array = np.array(train_array)\n",
    "    users, items = train_array[:, 0], train_array[:, 1]\n",
    "    row = np.concatenate([users, items + dataset.n_users], axis=0)\n",
    "    column = np.concatenate([items + dataset.n_users, users], axis=0)\n",
    "    adj_mat = sp.coo_matrix((np.ones(row.shape), np.stack([row, column], axis=0)),\n",
    "                            shape=(dataset.n_users + dataset.n_items, dataset.n_users + dataset.n_items),\n",
    "                            dtype=np.float32).tocsr()\n",
    "    return adj_mat\n",
    "\n",
    "def graph_rank_nodes(dataset, ranking_metric):\n",
    "    adj_mat = generate_daj_mat(dataset)\n",
    "    if ranking_metric == 'degree':\n",
    "        user_metrics = np.array(np.sum(adj_mat[:dataset.n_users, :], axis=1)).squeeze()\n",
    "        item_metrics = np.array(np.sum(adj_mat[dataset.n_users:, :], axis=1)).squeeze()\n",
    "    elif ranking_metric == 'greedy' or ranking_metric == 'sort':\n",
    "        '''\n",
    "        # This is for theoretical analysis.\n",
    "        part_adj = adj_mat[:dataset.n_users, dataset.n_users:]\n",
    "        part_adj_tensor = get_sparse_tensor(part_adj, 'cpu')\n",
    "        with torch.no_grad():\n",
    "            u, s, v = torch.svd_lowrank(part_adj_tensor, 64)\n",
    "            u, v = u.numpy(), v.numpy()\n",
    "        user_metrics = greedy_or_sort(part_adj, u, ranking_metric, dataset.device)\n",
    "        item_metrics = greedy_or_sort(part_adj.T, v, ranking_metric, dataset.device)\n",
    "        '''\n",
    "        normalized_adj_mat = normalize(adj_mat, axis=1, norm='l1')\n",
    "        user_metrics = np.array(np.sum(normalized_adj_mat[:, :dataset.n_users], axis=0)).squeeze()\n",
    "        item_metrics = np.array(np.sum(normalized_adj_mat[:, dataset.n_users:], axis=0)).squeeze()\n",
    "    elif ranking_metric == 'page_rank':\n",
    "        g = nx.Graph()\n",
    "        g.add_edges_from(np.array(np.nonzero(adj_mat)).T)\n",
    "        pr = nx.pagerank(g)\n",
    "        pr = np.array([pr[i] for i in range(dataset.n_users + dataset.n_items)])\n",
    "        user_metrics, item_metrics = pr[:dataset.n_users], pr[dataset.n_users:]\n",
    "    else:\n",
    "        return None\n",
    "    ranked_users = np.argsort(user_metrics)[::-1].copy()\n",
    "    ranked_items = np.argsort(item_metrics)[::-1].copy()\n",
    "    return ranked_users, ranked_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefb125-8cf5-4ede-9314-b36025958274",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model_config\n",
    "name = model_config['name']\n",
    "device = model_config['device']\n",
    "n_users = dataset.n_users\n",
    "n_items = dataset.n_items\n",
    "trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7588370-aef4-4954-a894-9587752303b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = model_config['embedding_size']\n",
    "n_layers = model_config['n_layers']\n",
    "dropout = model_config['dropout']\n",
    "feature_ratio = model_config['feature_ratio']  # Template%\n",
    "norm_adj = generate_graph(dataset)\n",
    "\n",
    "alpha = 1.\n",
    "delta = model_config.get('delta', 0.99)\n",
    "taugh = model_config.get('taugh', 0.2)\n",
    "aug_num = model_config['aug_num']\n",
    "# self.temper = model_config['temper']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba880a0-e1c4-4271-856a-9ef18d354e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat, user_map, item_map, row_sum = \\\n",
    "    generate_feat(dataset,\n",
    "                       ranking_metric=model_config.get('ranking_metric', 'sort'))\n",
    "#update_feat_mat()\n",
    "# self.norm_aug_adj = enerate_aug_graph(model_config['dataset'] )\n",
    "embedding = nn.Embedding(feat_mat.shape[1], embedding_size)\n",
    "\n",
    "w = nn.Parameter(torch.ones([embedding_size], dtype=torch.float32, device=device))\n",
    "normal_(embedding.weight, std=0.1)\n",
    "embedding.to(device=device)\n",
    "norm_aug_adj = generate_aug_graph(dataset)\n",
    "times = model_config.get('times', 0.1)\n",
    "# self.aug_num = (self.times) * len(self.feat_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2eec78-1e16-4fe8-bf22-ce4f27ec9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28d78f-1780-4ea0-81e5-f4fef72c5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494f34a-d143-42cb-bd96-4b87101f1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_aug_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8fb74a-02c6-4f86-b7c4-177eab109e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472ac5e-3cb2-494f-ae88-710349584f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=trainer_config['batch_size'],\n",
    "                                     num_workers=trainer_config['dataloader_num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502665c-6cac-4f4b-b52f-1981a7cf7c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dose",
   "language": "python",
   "name": "dose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
