{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c1a316-b89f-4819-97e6-aebcdddba301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "import networkx as nx\n",
    "from sortedcontainers import SortedList\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from utils import get_sparse_tensor, graph_rank_nodes, generate_daj_mat, generate_aug_daj_mat\n",
    "from torch.nn.init import kaiming_uniform_, xavier_normal, normal_, zeros_, ones_\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import dgl\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from info_nce import InfoNCE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import AverageMeter, get_sparse_tensor\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "from dataset import AuxiliaryDataset\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fc200e-1bad-4de1-a1ed-2337dcec623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_dataset\n",
    "from model import get_model\n",
    "from trainer import get_trainer\n",
    "import torch\n",
    "from utils import init_run\n",
    "from tensorboardX import SummaryWriter\n",
    "from config import get_gowalla_config, get_yelp_config, get_amazon_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a4b3fc-05cb-4699-a484-3774a3044221",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "config = get_gowalla_config(device)\n",
    "dataset_config, model_config, trainer_config = config[10]\n",
    "dataset_config['path'] = dataset_config['path'][:-4] + str(1)\n",
    "model_config['name'] = 'DOSE_aug'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d9f301-8384-4b70-9448-e313eece2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'ProcessedDataset', 'path': 'data/Gowalla/1', 'device': device(type='cuda'), 'neg_ratio': 4}\n",
      "init dataset ProcessedDataset\n",
      "{'name': 'DOSE_aug', 'embedding_size': 64, 'n_layers': 3, 'device': device(type='cuda'), 'dropout': 0.3, 'feature_ratio': 1, 'aug_num': 500000, 'dataset': <dataset.ProcessedDataset object at 0x14c18db59d00>}\n",
      "611909852\n"
     ]
    }
   ],
   "source": [
    "#writer = SummaryWriter(log_path)\n",
    "dataset = get_dataset(dataset_config) # datasetのクラス\n",
    "model = get_model(model_config, dataset) # modelのクラウス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a6ba827-f461-4ebb-84e6-5d0d865ffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f69c39a6-186f-4f55-873e-b4e090703663",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (1017545651.py, line 218)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [14], line 218\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.update_feat_mat()\u001b[0m\n\u001b[0m                          \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class DOSE(BasicModel):\n",
    "    def __init__(self, model_config):\n",
    "        super(DOSE, self).__init__(model_config)\n",
    "        self.embedding_size = model_config['embedding_size']\n",
    "        self.n_layers = model_config['n_layers']\n",
    "        self.dropout = model_config['dropout']\n",
    "        self.feature_ratio = model_config['feature_ratio']  # Template%\n",
    "        self.norm_adj = self.generate_graph(model_config['dataset'])\n",
    "       \n",
    "        self.alpha = 1.\n",
    "        self.delta = model_config.get('delta', 0.99)\n",
    "        self.taugh = model_config.get('taugh', 0.2)\n",
    "        self.aug_num = model_config['aug_num']\n",
    "        # self.temper = model_config['temper']\n",
    "        self.feat_mat, self.user_map, self.item_map, self.row_sum = \\\n",
    "            self.generate_feat(model_config['dataset'],\n",
    "                               ranking_metric=model_config.get('ranking_metric', 'sort'))\n",
    "        self.update_feat_mat()\n",
    "        # self.norm_aug_adj = enerate_aug_graph(model_config['dataset'] )\n",
    "        self.embedding = nn.Embedding(self.feat_mat.shape[1], self.embedding_size)\n",
    "        \n",
    "        self.w = nn.Parameter(torch.ones([self.embedding_size], dtype=torch.float32, device=self.device))\n",
    "        normal_(self.embedding.weight, std=0.1)\n",
    "        self.to(device=self.device)\n",
    "        self.norm_aug_adj = self.generate_aug_graph(model_config['dataset'])\n",
    "        self.times = model_config.get('times', 0.1)\n",
    "        # self.aug_num = (self.times) * len(self.feat_mat)\n",
    "\n",
    "    def update_feat_mat(self):\n",
    "        row, _ = self.feat_mat.indices()\n",
    "        edge_values = torch.pow(self.row_sum[row], (self.alpha - 1.) / 2. - 0.5)\n",
    "        self.feat_mat = torch.sparse.FloatTensor(self.feat_mat.indices(), edge_values, self.feat_mat.shape).coalesce()\n",
    "\n",
    "    def feat_mat_anneal(self):\n",
    "        self.alpha *= self.delta\n",
    "        self.update_feat_mat()\n",
    "\n",
    "    def generate_graph(self, dataset):\n",
    "        return LightGCN.generate_graph(self, dataset)\n",
    "    \n",
    "\n",
    "\n",
    "    def generate_aug_graph(self, dataset):\n",
    "        # new graph after adding some interactions\n",
    "        aug_idx = self.cal_cos_sim()\n",
    "        aug_adj_mat = generate_aug_daj_mat(dataset, aug_idx)\n",
    "        degree = np.array(np.sum(aug_adj_mat, axis=1)).squeeze()\n",
    "        degree = np.maximum(1., degree)\n",
    "        d_inv = np.power(degree, -0.5) # 累乗\n",
    "        d_mat = sp.diags(d_inv, format='csr', dtype=np.float32)\n",
    "\n",
    "        norm_aug_adj = d_mat.dot(aug_adj_mat).dot(d_mat)\n",
    "        norm_aug_adj = get_sparse_tensor(norm_aug_adj, self.device)\n",
    "        return norm_aug_adj\n",
    "\n",
    "    def generate_feat(self, dataset, is_updating=False, ranking_metric=None):\n",
    "        # return adj matrix with template\n",
    "        if not is_updating:\n",
    "            if self.feature_ratio < 1.:  # ランク付後にスライス\n",
    "\n",
    "                ranked_users, ranked_items = graph_rank_nodes(dataset, ranking_metric)\n",
    "                core_users = ranked_users[:int(self.n_users * self.feature_ratio)]\n",
    "                core_items = ranked_items[:int(self.n_items * self.feature_ratio)]\n",
    "            else:\n",
    "                core_users = np.arange(self.n_users, dtype=np.int64)\n",
    "                core_items = np.arange(self.n_items, dtype=np.int64)\n",
    "\n",
    "            user_map = dict()\n",
    "            for idx, user in enumerate(core_users):\n",
    "                user_map[user] = idx\n",
    "            item_map = dict()\n",
    "            for idx, item in enumerate(core_items):\n",
    "                item_map[item] = idx\n",
    "        else:\n",
    "            user_map = self.user_map\n",
    "            item_map = self.item_map\n",
    "\n",
    "        user_dim, item_dim = len(user_map), len(item_map)\n",
    "        indices = []\n",
    "        for user, item in dataset.train_array:\n",
    "            if item in item_map:\n",
    "                indices.append([user, user_dim + item_map[item]])\n",
    "            if user in user_map:\n",
    "                indices.append([self.n_users + item, user_map[user]])\n",
    "        for user in range(self.n_users):\n",
    "            indices.append([user, user_dim + item_dim])\n",
    "        for item in range(self.n_items):\n",
    "            indices.append([self.n_users + item, user_dim + item_dim + 1])\n",
    "        feat = sp.coo_matrix((np.ones((len(indices),)), np.array(indices).T),\n",
    "                             shape=(self.n_users + self.n_items, user_dim + item_dim + 2), dtype=np.float32).tocsr()\n",
    "        row_sum = torch.tensor(np.array(np.sum(feat, axis=1)).squeeze(), dtype=torch.float32, device=self.device)\n",
    "        feat = get_sparse_tensor(feat, self.device)\n",
    "        return feat, user_map, item_map, row_sum\n",
    "\n",
    "    def inductive_rep_layer(self, feat_mat):\n",
    "        # generate embedding by using template\n",
    "        padding_tensor = torch.empty([max(self.feat_mat.shape) - self.feat_mat.shape[1], self.embedding_size],\n",
    "                                     dtype=torch.float32, device=self.device)\n",
    "        padding_features = torch.cat([self.embedding.weight, padding_tensor], dim=0)\n",
    "\n",
    "        row, column = feat_mat.indices()\n",
    "        g = dgl.graph((column, row), num_nodes=max(self.feat_mat.shape), device=self.device)\n",
    "        x = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=padding_features, rhs_data=feat_mat.values())\n",
    "        x = x[:self.feat_mat.shape[0], :]\n",
    "        return x\n",
    "\n",
    "    def get_def_rep(self):\n",
    "        # generate final embedding\n",
    "        feat_mat = NGCF.dropout_sp_mat(self, self.feat_mat)\n",
    "        representations = self.inductive_rep_layer(feat_mat)\n",
    "\n",
    "        all_layer_rep = [representations]\n",
    "        row, column = self.norm_adj.indices()\n",
    "        g = dgl.graph((column, row), num_nodes=self.norm_adj.shape[0], device=self.device)\n",
    "        for _ in range(self.n_layers):\n",
    "            representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=self.norm_adj.values())\n",
    "            all_layer_rep.append(representations)\n",
    "        all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "        final_rep = all_layer_rep.mean(dim=0)\n",
    "        return final_rep\n",
    "\n",
    "    def get_aug_rep(self, norm_aug_adj):\n",
    "        # generate final embedding on aug-graph\n",
    "        feat_mat = NGCF.dropout_sp_mat(self, self.feat_mat)\n",
    "        representations = self.inductive_rep_layer(feat_mat)\n",
    "        \n",
    "        all_layer_rep = [representations]\n",
    "        row, column = norm_aug_adj.indices()\n",
    "        g = dgl.graph((column, row), num_nodes=norm_aug_adj.shape[0], device=self.device)\n",
    "        for _ in range(self.n_layers):\n",
    "            representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=norm_aug_adj.values())\n",
    "            all_layer_rep.append(representations)\n",
    "        all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "        final_rep = all_layer_rep.mean(dim=0)\n",
    "        return final_rep\n",
    "\n",
    "    def cal_cos_sim(self):\n",
    "        # calculate cosine similarity with user embeddings and item embeddings (on CPU)\n",
    "        rep = self.get_def_rep()\n",
    "        all_users_r = rep[:self.n_users, :]\n",
    "        all_items_r = rep[self.n_users:, :]\n",
    "        \n",
    "        all_users_r = all_users_r.to('cpu').detach().numpy().copy()\n",
    "        all_items_r = all_items_r.to('cpu').detach().numpy().copy()\n",
    "        \n",
    "        x = cosine_similarity(all_users_r, all_items_r)\n",
    "        cos_mat = torch.from_numpy(x.astype(np.float32)).clone()\n",
    "        cos_mat.to(device=self.device)\n",
    "        cos_sim = torch.reshape(cos_mat, (1, -1))\n",
    "        _, idx = torch.topk(cos_sim, self.aug_num)\n",
    "        #idx = idx.tolist()S\n",
    "        aug_idx = [[int(torch.div(idx[0][i], self.n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], self.n_items)))] for i in range(self.aug_num)]\n",
    "        \n",
    "        return aug_idx  # return list [user_id, item_id]\n",
    "    \n",
    "    def cal_cos_sim_v2(self):\n",
    "        # calculate cosine similarity with user embeddings and item embeddings (on GPU)\n",
    "        rep = self.get_def_rep()\n",
    "        all_users_r = rep[:self.n_users, :]\n",
    "        all_items_r = rep[self.n_users:, :]\n",
    "        \n",
    "        cos_sim = pairwise_cosine_similarity(all_users_r, all_items_r)\n",
    "        cos_sim = torch.reshape(cos_sim, (1, -1))\n",
    "        _, idx = torch.topk(cos_sim, self.aug_num)\n",
    "        aug_idx = [[int(torch.div(idx[0][i], self.n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], self.n_items)))] for i in range(self.aug_num)]\n",
    "\n",
    "        return aug_idx  # return list [user_id, item_id]\n",
    "\n",
    "\n",
    "    def cal_loss(self, users_r, aug_users_r):\n",
    "        # calcrate ssl-loss(InfoNCE)\n",
    "        loss = InfoNCE(negative_mode='paired')\n",
    "        \n",
    "        query = users_r / self.taugh\n",
    "        positive_key = aug_users_r\n",
    "        negative_keys = aug_users_r.unsqueeze(1)\n",
    "        \n",
    "        contrasive_loss = loss(query, positive_key, negative_keys)\n",
    "        return contrasive_loss\n",
    "\n",
    "    def bpr_forward(self, users, pos_items, neg_items):\n",
    "        # 普通の埋め込み\n",
    "        rep = self.get_def_rep()\n",
    "        users_r = rep[users, :]\n",
    "        pos_items_r, neg_items_r = rep[self.n_users + pos_items, :], rep[self.n_users + neg_items, :]\n",
    "        # AUG後の埋め込み\n",
    "        \n",
    "        aug_rep = self.get_aug_rep(self.norm_aug_adj)\n",
    "        aug_users_r = aug_rep[users, :]\n",
    "        l2_norm_sq = torch.norm(users_r, p=2, dim=1) ** 2 + torch.norm(pos_items_r, p=2, dim=1) ** 2 \\\n",
    "                     + torch.norm(neg_items_r, p=2, dim=1) ** 2\n",
    "        # Contrasive loss\n",
    "        contrasive_loss = self.cal_loss(users_r, aug_users_r)\n",
    "        return users_r, pos_items_r, neg_items_r, l2_norm_sq, contrasive_loss\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, users):\n",
    "        rep = self.get_def_rep()\n",
    "        users_r = rep[users, :]\n",
    "        all_items_r = rep[self.n_users:, :]\n",
    "        scores = torch.mm(users_r, all_items_r.t())\n",
    "        return scores\n",
    "    \n",
    "    def save(self, path):\n",
    "        params = {'sate_dict': self.state_dict(), 'user_map': self.user_map,\n",
    "                  'item_map': self.item_map, 'alpha': self.alpha}\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        params = torch.load(path, map_location=self.device)\n",
    "        self.load_state_dict(params['sate_dict'])\n",
    "        self.user_map = params['user_map']\n",
    "        self.item_map = params['item_map']\n",
    "        self.alpha = params['alpha']\n",
    "        self.feat_mat, _, _, self.row_sum = self.generate_feat(self.config['dataset'], is_updating=True)\n",
    "        self.update_feat_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "685eef4c-bf6b-46ba-8b6f-466beb6ec815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_feat_mat():\n",
    "    row, _ = feat_mat.indices()\n",
    "    edge_values = torch.pow(row_sum[row], (alpha - 1.) / 2. - 0.5)\n",
    "    feat_mat = torch.sparse.FloatTensor(feat_mat.indices(), edge_values, feat_mat.shape).coalesce()\n",
    "\n",
    "def feat_mat_anneal():\n",
    "    alpha *= delta\n",
    "    self.update_feat_mat()\n",
    "\n",
    "\n",
    "def generate_graph( dataset):\n",
    "    adj_mat = generate_daj_mat(dataset)\n",
    "    degree = np.array(np.sum(adj_mat, axis=1)).squeeze()\n",
    "    degree = np.maximum(1., degree)\n",
    "    d_inv = np.power(degree, -0.5) # 累乗\n",
    "    d_mat = sp.diags(d_inv, format='csr', dtype=np.float32)\n",
    "\n",
    "    norm_adj = d_mat.dot(adj_mat).dot(d_mat)\n",
    "    norm_adj = get_sparse_tensor(norm_adj, device)\n",
    "    return norm_adj\n",
    "\n",
    "\n",
    "\n",
    "def generate_aug_graph(dataset):\n",
    "    # new graph after adding some interactions\n",
    "    aug_idx = cal_cos_sim()\n",
    "    aug_adj_mat = generate_aug_daj_mat(dataset, aug_idx)\n",
    "    degree = np.array(np.sum(aug_adj_mat, axis=1)).squeeze()\n",
    "    degree = np.maximum(1., degree)\n",
    "    d_inv = np.power(degree, -0.5) # 累乗\n",
    "    d_mat = sp.diags(d_inv, format='csr', dtype=np.float32)\n",
    "\n",
    "    norm_aug_adj = d_mat.dot(aug_adj_mat).dot(d_mat)\n",
    "    norm_aug_adj = get_sparse_tensor(norm_aug_adj, device)\n",
    "    return norm_aug_adj\n",
    "\n",
    "def generate_feat(dataset, is_updating=False, ranking_metric=None):\n",
    "    # return adj matrix with template\n",
    "    if not is_updating:\n",
    "        if feature_ratio < 1.:  # ランク付後にスライス\n",
    "\n",
    "            ranked_users, ranked_items = graph_rank_nodes(dataset, ranking_metric)\n",
    "            core_users = ranked_users[:int(n_users * feature_ratio)]\n",
    "            core_items = ranked_items[:int(n_items * feature_ratio)]\n",
    "        else:\n",
    "            core_users = np.arange(n_users, dtype=np.int64)\n",
    "            core_items = np.arange(n_items, dtype=np.int64)\n",
    "\n",
    "        user_map = dict()\n",
    "        for idx, user in enumerate(core_users):\n",
    "            user_map[user] = idx\n",
    "        item_map = dict()\n",
    "        for idx, item in enumerate(core_items):\n",
    "            item_map[item] = idx\n",
    "    else:\n",
    "        user_map = user_map\n",
    "        item_map = item_map\n",
    "\n",
    "    user_dim, item_dim = len(user_map), len(item_map)\n",
    "    indices = []\n",
    "    for user, item in dataset.train_array:\n",
    "        if item in item_map:\n",
    "            indices.append([user, user_dim + item_map[item]])\n",
    "        if user in user_map:\n",
    "            indices.append([n_users + item, user_map[user]])\n",
    "    for user in range(n_users):\n",
    "        indices.append([user, user_dim + item_dim])\n",
    "    for item in range(n_items):\n",
    "        indices.append([n_users + item, user_dim + item_dim + 1])\n",
    "    feat = sp.coo_matrix((np.ones((len(indices),)), np.array(indices).T),\n",
    "                         shape=(n_users + n_items, user_dim + item_dim + 2), dtype=np.float32).tocsr()\n",
    "    row_sum = torch.tensor(np.array(np.sum(feat, axis=1)).squeeze(), dtype=torch.float32, device=device)\n",
    "    feat = get_sparse_tensor(feat, device)\n",
    "    return feat, user_map, item_map, row_sum\n",
    "\n",
    "def inductive_rep_layer(feat_mat):\n",
    "    # generate embedding by using template\n",
    "    padding_tensor = torch.empty([max(feat_mat.shape) - feat_mat.shape[1], embedding_size],\n",
    "                                 dtype=torch.float32, device=device)\n",
    "    padding_features = torch.cat([embedding.weight, padding_tensor], dim=0)\n",
    "\n",
    "    row, column = feat_mat.indices()\n",
    "    g = dgl.graph((column, row), num_nodes=max(feat_mat.shape), device=device)\n",
    "    x = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=padding_features, rhs_data=feat_mat.values())\n",
    "    x = x[:feat_mat.shape[0], :]\n",
    "    return x\n",
    "\n",
    "def get_def_rep():\n",
    "    # generate final embedding\n",
    "    #feat_mat = NGCF.dropout_sp_mat(feat_mat)\n",
    "    representations = inductive_rep_layer(feat_mat)\n",
    "\n",
    "    all_layer_rep = [representations]\n",
    "    row, column = norm_adj.indices()\n",
    "    g = dgl.graph((column, row), num_nodes=norm_adj.shape[0], device=device)\n",
    "    for _ in range(n_layers):\n",
    "        representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=norm_adj.values())\n",
    "        all_layer_rep.append(representations)\n",
    "    all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "    final_rep = all_layer_rep.mean(dim=0)\n",
    "    return final_rep\n",
    "\n",
    "def get_aug_rep(norm_aug_adj):\n",
    "    # generate final embedding on aug-graph\n",
    "    #feat_mat = NGCF.dropout_sp_mat(self, self.feat_mat)\n",
    "    representations = inductive_rep_layer(feat_mat)\n",
    "\n",
    "    all_layer_rep = [representations]\n",
    "    row, column = norm_aug_adj.indices()\n",
    "    g = dgl.graph((column, row), num_nodes=norm_aug_adj.shape[0], device=device)\n",
    "    for _ in range(n_layers):\n",
    "        representations = dgl.ops.gspmm(g, 'mul', 'sum', lhs_data=representations, rhs_data=norm_aug_adj.values())\n",
    "        all_layer_rep.append(representations)\n",
    "    all_layer_rep = torch.stack(all_layer_rep, dim=0)\n",
    "    final_rep = all_layer_rep.mean(dim=0)\n",
    "    return final_rep\n",
    "\n",
    "def cal_cos_sim():\n",
    "    # calculate cosine similarity with user embeddings and item embeddings (on CPU)\n",
    "    rep = get_def_rep()\n",
    "    all_users_r = rep[:n_users, :]\n",
    "    all_items_r = rep[n_users:, :]\n",
    "\n",
    "    all_users_r = all_users_r.to('cpu').detach().numpy().copy()\n",
    "    all_items_r = all_items_r.to('cpu').detach().numpy().copy()\n",
    "\n",
    "    x = cosine_similarity(all_users_r, all_items_r)\n",
    "    cos_mat = torch.from_numpy(x.astype(np.float32)).clone()\n",
    "    cos_mat.to(device=device)\n",
    "    cos_sim = torch.reshape(cos_mat, (1, -1))\n",
    "    _, idx = torch.topk(cos_sim, aug_num)\n",
    "    #idx = idx.tolist()S\n",
    "    aug_idx = [[int(torch.div(idx[0][i], n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], n_items)))] for i in range(aug_num)]\n",
    "\n",
    "    return aug_idx  # return list [user_id, item_id]\n",
    "\n",
    "def cal_cos_sim_v2():\n",
    "    # calculate cosine similarity with user embeddings and item embeddings (on GPU)\n",
    "    rep = get_def_rep()\n",
    "    all_users_r = rep[:n_users, :]\n",
    "    all_items_r = rep[n_users:, :]\n",
    "\n",
    "    cos_sim = pairwise_cosine_similarity(all_users_r, all_items_r)\n",
    "    cos_sim = torch.reshape(cos_sim, (1, -1))\n",
    "    _, idx = torch.topk(cos_sim, aug_num)\n",
    "    aug_idx = [[int(torch.div(idx[0][i], n_items, rounding_mode='floor')), (int(torch.fmod(idx[0][i], n_items)))] for i in range(aug_num)]\n",
    "\n",
    "    return aug_idx  # return list [user_id, item_id]\n",
    "\n",
    "\n",
    "def cal_loss(users_r, aug_users_r):\n",
    "    # calcrate ssl-loss(InfoNCE)\n",
    "    loss = InfoNCE(negative_mode='unpaired')\n",
    "\n",
    "    query = users_r / taugh\n",
    "    positive_key = aug_users_r\n",
    "    negative_keys = aug_users_r\n",
    "\n",
    "    contrasive_loss = loss(query, positive_key, negative_keys)\n",
    "    return contrasive_loss\n",
    "\n",
    "def bpr_forward(self, users, pos_items, neg_items):\n",
    "    # 普通の埋め込み\n",
    "    rep = get_def_rep()\n",
    "    users_r = rep[users, :]\n",
    "    pos_items_r, neg_items_r = rep[n_users + pos_items, :], rep[n_users + neg_items, :]\n",
    "    # AUG後の埋め込み\n",
    "\n",
    "    aug_rep = get_aug_rep(norm_aug_adj)\n",
    "    aug_users_r = aug_rep[users, :]\n",
    "    l2_norm_sq = torch.norm(users_r, p=2, dim=1) ** 2 + torch.norm(pos_items_r, p=2, dim=1) ** 2 \\\n",
    "                 + torch.norm(neg_items_r, p=2, dim=1) ** 2\n",
    "    # Contrasive loss\n",
    "    contrasive_loss = cal_loss(users_r, aug_users_r)\n",
    "    return users_r, pos_items_r, neg_items_r, l2_norm_sq, contrasive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ad38a8-73ef-49c3-b476-6c121a51d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_tensor(mat, device):\n",
    "    coo = mat.tocoo()\n",
    "    indexes = np.stack([coo.row, coo.col], axis=0)\n",
    "    indexes = torch.tensor(indexes, dtype=torch.int64, device=device)\n",
    "    data = torch.tensor(coo.data, dtype=torch.float32, device=device)\n",
    "    sp_tensor = torch.sparse.FloatTensor(indexes, data, torch.Size(coo.shape)).coalesce()\n",
    "    return sp_tensor\n",
    "\n",
    "\n",
    "def generate_daj_mat(dataset): # Sparseなadj\n",
    "    train_array = np.array(dataset.train_array)\n",
    "    users, items = train_array[:, 0], train_array[:, 1]\n",
    "    row = np.concatenate([users, items + dataset.n_users], axis=0)\n",
    "    column = np.concatenate([items + dataset.n_users, users], axis=0)\n",
    "    adj_mat = sp.coo_matrix((np.ones(row.shape), np.stack([row, column], axis=0)),\n",
    "                            shape=(dataset.n_users + dataset.n_items, dataset.n_users + dataset.n_items),\n",
    "                            dtype=np.float32).tocsr()\n",
    "    return adj_mat\n",
    "def generate_aug_daj_mat(dataset, aug_idx): # Sparseなadj\n",
    "    # generate adj matrix on aug-graph\n",
    "    train_array = dataset.train_array\n",
    "    train_array.extend(aug_idx)\n",
    "    train_array = np.array(train_array)\n",
    "    users, items = train_array[:, 0], train_array[:, 1]\n",
    "    row = np.concatenate([users, items + dataset.n_users], axis=0)\n",
    "    column = np.concatenate([items + dataset.n_users, users], axis=0)\n",
    "    adj_mat = sp.coo_matrix((np.ones(row.shape), np.stack([row, column], axis=0)),\n",
    "                            shape=(dataset.n_users + dataset.n_items, dataset.n_users + dataset.n_items),\n",
    "                            dtype=np.float32).tocsr()\n",
    "    return adj_mat\n",
    "\n",
    "def graph_rank_nodes(dataset, ranking_metric):\n",
    "    adj_mat = generate_daj_mat(dataset)\n",
    "    if ranking_metric == 'degree':\n",
    "        user_metrics = np.array(np.sum(adj_mat[:dataset.n_users, :], axis=1)).squeeze()\n",
    "        item_metrics = np.array(np.sum(adj_mat[dataset.n_users:, :], axis=1)).squeeze()\n",
    "    elif ranking_metric == 'greedy' or ranking_metric == 'sort':\n",
    "        '''\n",
    "        # This is for theoretical analysis.\n",
    "        part_adj = adj_mat[:dataset.n_users, dataset.n_users:]\n",
    "        part_adj_tensor = get_sparse_tensor(part_adj, 'cpu')\n",
    "        with torch.no_grad():\n",
    "            u, s, v = torch.svd_lowrank(part_adj_tensor, 64)\n",
    "            u, v = u.numpy(), v.numpy()\n",
    "        user_metrics = greedy_or_sort(part_adj, u, ranking_metric, dataset.device)\n",
    "        item_metrics = greedy_or_sort(part_adj.T, v, ranking_metric, dataset.device)\n",
    "        '''\n",
    "        normalized_adj_mat = normalize(adj_mat, axis=1, norm='l1')\n",
    "        user_metrics = np.array(np.sum(normalized_adj_mat[:, :dataset.n_users], axis=0)).squeeze()\n",
    "        item_metrics = np.array(np.sum(normalized_adj_mat[:, dataset.n_users:], axis=0)).squeeze()\n",
    "    elif ranking_metric == 'page_rank':\n",
    "        g = nx.Graph()\n",
    "        g.add_edges_from(np.array(np.nonzero(adj_mat)).T)\n",
    "        pr = nx.pagerank(g)\n",
    "        pr = np.array([pr[i] for i in range(dataset.n_users + dataset.n_items)])\n",
    "        user_metrics, item_metrics = pr[:dataset.n_users], pr[dataset.n_users:]\n",
    "    else:\n",
    "        return None\n",
    "    ranked_users = np.argsort(user_metrics)[::-1].copy()\n",
    "    ranked_items = np.argsort(item_metrics)[::-1].copy()\n",
    "    return ranked_users, ranked_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaefb125-8cf5-4ede-9314-b36025958274",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model_config\n",
    "name = model_config['name']\n",
    "device = model_config['device']\n",
    "n_users = dataset.n_users\n",
    "n_items = dataset.n_items\n",
    "trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7588370-aef4-4954-a894-9587752303b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = model_config['embedding_size']\n",
    "n_layers = model_config['n_layers']\n",
    "dropout = model_config['dropout']\n",
    "feature_ratio = model_config['feature_ratio']  # Template%\n",
    "norm_adj = generate_graph(dataset)\n",
    "\n",
    "alpha = 1.\n",
    "delta = model_config.get('delta', 0.99)\n",
    "taugh = model_config.get('taugh', 0.2)\n",
    "aug_num = model_config['aug_num']\n",
    "# self.temper = model_config['temper']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba880a0-e1c4-4271-856a-9ef18d354e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mat, user_map, item_map, row_sum = \\\n",
    "    generate_feat(dataset,\n",
    "                       ranking_metric=model_config.get('ranking_metric', 'sort'))\n",
    "#update_feat_mat()\n",
    "# self.norm_aug_adj = enerate_aug_graph(model_config['dataset'] )\n",
    "embedding = nn.Embedding(feat_mat.shape[1], embedding_size)\n",
    "\n",
    "w = nn.Parameter(torch.ones([embedding_size], dtype=torch.float32, device=device))\n",
    "normal_(embedding.weight, std=0.1)\n",
    "embedding.to(device=device)\n",
    "norm_aug_adj = generate_aug_graph(dataset)\n",
    "times = model_config.get('times', 0.1)\n",
    "# self.aug_num = (self.times) * len(self.feat_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c2eec78-1e16-4fe8-bf22-ce4f27ec9d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 70845, 70845, 70845],\n",
       "                       [17914, 17918, 17922,  ..., 16861, 17305, 42507]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       device='cuda:0', size=(70846, 42508), nnz=2028052,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb28d78f-1780-4ea0-81e5-f4fef72c5913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 70845, 70845, 70845],\n",
       "                       [29858, 29859, 29861,  ..., 26841, 28266, 29001]]),\n",
       "       values=tensor([0.0163, 0.0057, 0.0119,  ..., 0.0173, 0.0541, 0.0358]),\n",
       "       device='cuda:0', size=(70846, 70846), nnz=2286148,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6494f34a-d143-42cb-bd96-4b87101f1778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 70845, 70845, 70845],\n",
       "                       [29858, 29859, 29861,  ..., 26860, 28266, 29001]]),\n",
       "       values=tensor([0.0102, 0.0048, 0.0096,  ..., 0.0570, 0.0489, 0.0333]),\n",
       "       device='cuda:0', size=(70846, 70846), nnz=3142568,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_aug_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f8fb74a-02c6-4f86-b7c4-177eab109e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aug_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maug_idx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aug_idx' is not defined"
     ]
    }
   ],
   "source": [
    "aug_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0472ac5e-3cb2-494f-ae88-710349584f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=trainer_config['batch_size'],\n",
    "                                     num_workers=trainer_config['dataloader_num_workers'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38aa906-6bd3-4b92-b397-7f915a8e0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data in dataloader:\n",
    "    #start_time1 = time.time()\n",
    "    inputs = batch_data[:, 0, :].to(device=device, dtype=torch.int64)\n",
    "    users, pos_items, neg_items = inputs[:, 0],  inputs[:, 1],  inputs[:, 2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502665c-6cac-4f4b-b52f-1981a7cf7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_r = get_def_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb6ebdf-ffc7-4ce2-bbe5-85eb9b426e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d12c3c-f3b8-4fc9-9be5-ccd9e7f2b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_r = get_aug_rep(norm_aug_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34cf5cfd-e127-45d3-bbf0-5222a39ef2f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3381987239.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"for batch_data in dataloader:\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for batch_data in dataloader:\n",
    "    inputs = batch_data[:, 0, :].to(device=device, dtype=torch.int64)\n",
    "    users, pos_items, neg_items = inputs[:, 0],  inputs[:, 1],  inputs[:, 2]\n",
    "    user_r = all_r[users, :]\n",
    "    print(user_r.sort())\n",
    "    aug_user_r = aug_r[users, :]\n",
    "    loss = cal_loss(user_r, aug_user_r)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bdf389-cb84-48c7-945a-5e79531d1cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bach_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mbach_data\u001b[49m[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m      2\u001b[0m users, pos_items, neg_items \u001b[38;5;241m=\u001b[39m inputs[:, \u001b[38;5;241m0\u001b[39m],  inputs[:, \u001b[38;5;241m1\u001b[39m],  inputs[:, \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bach_data' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = bach_data[:, 0, :].to(device=device, dtype=torch.int64)\n",
    "users, pos_items, neg_items = inputs[:, 0],  inputs[:, 1],  inputs[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7561278b-9981-4b48-9249-beb0b427eaef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user_r \u001b[38;5;241m=\u001b[39m \u001b[43mall_r\u001b[49m[users, :]\n\u001b[1;32m      2\u001b[0m aug_user_r \u001b[38;5;241m=\u001b[39m aug_r[users, :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_r' is not defined"
     ]
    }
   ],
   "source": [
    "user_r = all_r[users, :]\n",
    "aug_user_r = aug_r[users, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0895fd84-0991-4d6f-adf3-6685bb611bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([296, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_r.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1591b61b-2849-4572-a3ce-d2a8be65a9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([296, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_user_r.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ddd8e-50de-403c-9989-d8df8d85e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cal_loss(user_r, aug_user_r)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb2d00-dcc4-4570-bf04-1bcad6f893a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b08a92-7c38-4335-957f-518534700b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae6dd3-6a50-434d-9811-1d91794496c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97aab0-aeef-4b33-88cf-ccb4651bd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding(pos_items + len(user_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a05134ff-d700-495a-b3de-7e89b5007075",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_items_r, neg_items_r = all_r[n_users + pos_items, :], all_r[n_users + neg_items, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6f92dd8-32c3-4508-9553-79813d1a0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_r, pos_items_r, neg_items_r, l2_norm_sq, contrastive_loss = self.model.bpr_forward(users, pos_items, neg_items)\n",
    "pos_scores = torch.sum(user_r * pos_items_r, dim=1)\n",
    "neg_scores = torch.sum(user_r * neg_items_r, dim=1)\n",
    "bpr_loss = F.softplus(neg_scores - pos_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beefe549-6f09-4e07-a2ed-c4372f2194f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4619, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e00e280-0e93-4e74-90a6-029fd23bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = bpr_loss + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99df7922-6175-4a35-8ff3-6a244827252b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1551, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "862105fe-e603-451b-841b-8fe1070fef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_metric = 'degree'\n",
    "ranked_users, ranked_items = graph_rank_nodes(dataset, ranking_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4edfb54-8663-497b-8e0e-6fee13e7fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  745,  2751,   820, ..., 13678, 20115, 18270])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dc4f182-88b2-465c-8a52-b434763e000f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0823,  0.3671,  0.0626,  ..., -0.7633,  0.0223, -1.5940],\n",
       "        [ 1.3623,  1.1896,  0.0042,  ..., -2.2789,  2.1133,  0.4617],\n",
       "        [ 1.6749, -0.0745,  0.7189,  ..., -0.6808,  0.8492, -0.6798],\n",
       "        ...,\n",
       "        [ 0.0543,  0.1464, -0.0403,  ..., -0.0886,  0.1143,  0.1149],\n",
       "        [-0.0938, -0.2187,  0.0548,  ..., -0.0205,  0.1147,  0.0090],\n",
       "        [ 0.0404, -0.0958,  0.0739,  ..., -0.0384, -0.0439,  0.0160]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_r[ranked_users, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bfac34e-1559-40c0-9bf2-9984eb26d182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40988, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_r[n_users+ranked_items, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c788308-5f07-4fba-95ff-95640c0b1ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40988"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ranked_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fa1532-c3cb-4437-9cb5-95fdbf1f8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5255c26d-c529-4d8b-9e90-c1bfec03129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = np.array([1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cef81c6-6597-4400-b4a5-c9281914829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist2 = mylist.reshape(2, 4)\n",
    "mylist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ac377c-4d2a-4752-b0db-b048a80377dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist3 = mylist2.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51969831-759b-4d20-b787-468c81218e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6, 7, 8]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb18e6-ee3b-4998-b243-443af8b4825c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dose",
   "language": "python",
   "name": "dose"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
